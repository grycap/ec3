
description slurm (
    kind = 'main' and
    short = 'Install and configure a cluster SLURM 14.11 from source code.' and
    content = 'The template installs SLURM 14.11 from source code. Initially the template creates as many working node hostnames as the sum of the values of feature "ec3_max_instances_max" in every system.

Webpage: http://slurm.schedmd.com/'
)

network public (
   outbound = 'yes' and 
   outports contains '6818/tcp' and
   outports contains '6817/tcp'
)

network private ()

system front (
   cpu.count>=1 and
   memory.size>=512m and
   net_interface.0.connection = 'public' and
   net_interface.0.dns_name = 'slurmserverpublic' and
   net_interface.1.connection = 'private' and
   net_interface.1.dns_name = 'slurmserver' and
   queue_system = 'slurm' and
   ec3_templates contains 'slurm'
)

system wn (
  net_interface.0.connection='private'
)

configure slurm_source (
@begin
    #SLURM dependences 
    - name: update repositories cache and apt install slurm dependences in Deb systems
      apt: name=libmunge2,libpq5,openssl-blacklist,munge,libmunge-dev,gcc,make  update_cache=yes cache_valid_time=3600
      when: ansible_os_family == "Debian"

    - name: update repositories cache and yum install slurm dependences in REL systems
      yum: name=readline-devel,openssl-devel,munge-devel,pam-devel,perl-ExtUtils-MakeMaker,gcc,make
      when: ansible_os_family == "RedHat"

    - name: download SLURM package
      get_url: url=http://www.schedmd.com/download/archive/slurm-14.11.3.tar.bz2 dest=/tmp/slurm.tar.bz2

    - name: decompress SLURM package
      command: tar xjf slurm.tar.bz2 chdir=/tmp creates=/tmp/slurm-14.11.3

    - name: configure SLURM
      command: ./configure {{ "--with-blcr=/usr/local/bin" if "blcr" in TEMPLATES else "" }} chdir=/tmp/slurm-14.11.3 creates=/tmp/slurm-14.11.3/config.log

    - name: compile (make) SLURM
      command: make chdir=/tmp/slurm-14.11.3 creates=/tmp/slurm-14.11.3/src/slurmctld

    - name: install SLURM
      command: make install chdir=/tmp/slurm-14.11.3 creates=/usr/local/bin/srun
@end
)

configure slurm_source_wn (
@begin
    - file: path=/usr/local/{{item}} state=directory owner=root group=root
      with_items: [ "bin", "sbin", "lib", "include", "libexec" ]
    - name: install SLURM
      copy: src=/usr/local/{{item}} dest=/usr/local/{{item}} mode=0755
      with_items: [ "bin/srun", "sbin/slurmd", "sbin/slurmstepd" ]
    - copy: src=/usr/local/bin/srun_cr dest=/usr/local/bin/srun_cr mode=0755
      ignore_errors: yes
    - copy: src=/usr/local/{{item}} dest=/usr/local/{{item}}
      with_items: [ "include/slurm/", "lib/libpmi.so", "lib/libslurm.so", "lib/slurm/", "libexec/slurm/" ]
@end
)

configure slurm_common (
@begin
    - name: create slurm user and group
      user: name=slurm shell=/bin/bash system=yes

    - name: create folders used by SLURM and set slurm owner
      file: path={{item}} state=directory owner=slurm group=slurm
      with_items:
         - /var/spool/slurm
         - /var/log/slurm
         - /var/slurm/checkpoint
@end
)

configure wn (
@begin
---
  - ec3_prio: -5
    vars:
      TEMPLATES:
        ec3_jpath: /system/front/ec3_templates
    tasks:
    - include: munge_repo_wn.yml
    - include: slurm_source.yml
    - include: slurm_common.yml

    - name: copy slurm.conf file from the frontend
      copy: src=/usr/local/etc/slurm.conf dest=/usr/local/etc/slurm.conf

    - name: start slurmd daemon
      shell: pgrep slurmd || slurmd
      #command: slurmd
    - shell: scontrol reconfig
    - local_action: shell scontrol update NodeName={{IM_NODE_HOSTNAME}} State=RESUME
      ignore_errors: yes
@end
)

include slurm_misc (
  template = 'clues2 im munge'
)

configure front (
@begin
---
  - ec3_prio: -5
    vars:
      FILE:
        ec3_file: is_cluster_ready
      SYSTEMS:
        ec3_jpath: /system/*
      SLURM_CONF_FILE:
        ec3_file: slurm.conf
      TEMPLATES:
        ec3_jpath: /system/front/ec3_templates

    tasks:
    - copy:
        dest: /bin/is_cluster_ready
        content: "{{FILE}}"
        mode: 0755
    - name: create epel.repo
      action: template src=utils/templates/epel-es.repo dest=/etc/yum.repos.d/epel.repo
      when: ansible_os_family == "RedHat"

    # Manage the /etc/hosts file
    - shell: |
        for i in `seq 0 {{item.ec3_max_instances_max|int - 1}}`; do
          item="{{item.id|replace('_','x')}}${i}";
          grep -q "\<${item}\>" /etc/hosts || echo "127.0.0.1 ${item} ${item}.localdomain" >> /etc/hosts;
        done
      with_items: SYSTEMS | selectattr("ec3_max_instances_max", "defined") | list

    #- include: im_devel_git.yml
    - include: im_devel_pip.yml
    - include: munge_repo_front.yml
    - include: slurm_source.yml
    - include: slurm_common.yml

    - copy:
        dest: /usr/local/etc/slurm.conf
        content: "{{SLURM_CONF_FILE}}"

    - lineinfile: dest=/usr/local/etc/slurm.conf regexp='#CheckpointType=' line='CheckpointType=checkpoint/blcr' state=present
      when: "'blcr' in TEMPLATES"
    - lineinfile: dest=/usr/local/etc/slurm.conf regexp='ControlMachine=' line='ControlMachine={{IM_NODE_HOSTNAME}}' state=present
    - name: configure compute nodes section of slurm.conf
      lineinfile: dest=/usr/local/etc/slurm.conf regexp='NodeName={{item.id|replace('_','x')}}\[' line='NodeName={{item.id|replace('_','x')}}[0-{{item.ec3_max_instances_max|int - 1}}] CPUs=1 State=UNKNOWN' state=present
      with_items: SYSTEMS | selectattr("ec3_max_instances_max", "defined") | list
    - lineinfile: dest=/usr/local/etc/slurm.conf regexp='PartitionName={{item.id}} ' line='PartitionName={{item.id}} Nodes={{item.id|replace('_','x')}}[0-{{item.ec3_max_instances_max|int - 1}}] MaxTime=INFINITE State=UP' state=present
      with_items: SYSTEMS | selectattr("ec3_max_instances_max", "defined") | list

    # start SLURM slurmctld daemon
    #- command: slurmctld
    - shell: pgrep slurmctld || nohup /usr/local/sbin/slurmctld > /dev/null 2>&1 &
    - shell: scontrol reconfig
@end
)

deploy front 1
